{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Heart Disease Analysis and Prediction\n",
        "\n",
        "This notebook performs comprehensive analysis and prediction on heart disease datasets using machine learning models.\n",
        "\n",
        "## Overview\n",
        "- Data preprocessing and exploration\n",
        "- Feature analysis and visualization\n",
        "- Model training and evaluation\n",
        "- Model explanation and interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install numpy pandas matplotlib seaborn scikit-learn shap xgboost lightgbm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization styles\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Data Loading\n",
        "\n",
        "# For Google Colab: Upload datasets\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload the heart disease datasets (heart.csv, heart_disease_uci.csv, heart_cleveland_upload.csv)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Alternatively, you can download from GitHub if datasets are available there\n",
        "# !wget https://raw.githubusercontent.com/your-username/heart-disease-project/main/heart.csv\n",
        "# !wget https://raw.githubusercontent.com/your-username/heart-disease-project/main/heart_disease_uci.csv\n",
        "# !wget https://raw.githubusercontent.com/your-username/heart-disease-project/main/heart_cleveland_upload.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "heart_df = pd.read_csv('heart.csv')\n",
        "uci_df = pd.read_csv('heart_disease_uci.csv')\n",
        "cleveland_df = pd.read_csv('heart_cleveland_upload.csv')\n",
        "\n",
        "# Display basic information about datasets\n",
        "print(\"Heart Dataset:\")\n",
        "print(f\"Shape: {heart_df.shape}\")\n",
        "heart_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUCI Dataset:\")\n",
        "print(f\"Shape: {uci_df.shape}\")\n",
        "uci_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCleveland Dataset:\")\n",
        "print(f\"Shape: {cleveland_df.shape}\")\n",
        "cleveland_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Data Preprocessing and Exploration\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in Heart Dataset:\")\n",
        "print(heart_df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in UCI Dataset:\")\n",
        "print(uci_df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in Cleveland Dataset:\")\n",
        "print(cleveland_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function for data preprocessing\n",
        "def preprocess_data(df, dataset_name):\n",
        "    \"\"\"\n",
        "    Preprocess dataset and return cleaned dataframe\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    data = df.copy()\n",
        "    \n",
        "    # Handle missing values if any (based on the dataset)\n",
        "    if dataset_name == 'uci':\n",
        "        # Fill missing values with median for numerical columns\n",
        "        for col in data.select_dtypes(include=['float64', 'int64']).columns:\n",
        "            if data[col].isnull().sum() > 0:\n",
        "                data[col] = data[col].fillna(data[col].median())\n",
        "    \n",
        "    # Convert target variable to binary (0 or 1) if needed\n",
        "    if dataset_name == 'cleveland':\n",
        "        if 'condition' in data.columns:\n",
        "            data['target'] = data['condition'].apply(lambda x: 0 if x == 0 else 1)\n",
        "        elif 'target' in data.columns and data['target'].max() > 1:\n",
        "            data['target'] = data['target'].apply(lambda x: 0 if x == 0 else 1)\n",
        "    \n",
        "    # Create common feature names for all datasets\n",
        "    # This would depend on your specific datasets\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Preprocess all datasets\n",
        "heart_processed = preprocess_data(heart_df, 'heart')\n",
        "uci_processed = preprocess_data(uci_df, 'uci')\n",
        "cleveland_processed = preprocess_data(cleveland_df, 'cleveland')\n",
        "\n",
        "print(\"Processed datasets shapes:\")\n",
        "print(f\"Heart: {heart_processed.shape}\")\n",
        "print(f\"UCI: {uci_processed.shape}\")\n",
        "print(f\"Cleveland: {cleveland_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "# Create histograms for key numerical features in the Heart dataset\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    if col in heart_processed.columns:\n",
        "        sns.histplot(data=heart_processed, x=col, hue='target', bins=20, kde=True, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {col} by Target')\n",
        "        axes[i].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Numerical Features Distribution by Target - Heart Dataset', fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create boxplots for numerical features to detect outliers\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    if col in heart_processed.columns:\n",
        "        sns.boxplot(data=heart_processed, x='target', y=col, ax=axes[i])\n",
        "        axes[i].set_title(f'Boxplot of {col} by Target')\n",
        "        axes[i].set_ylabel(col)\n",
        "        axes[i].set_xlabel('Heart Disease')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Boxplots of Numerical Features by Target - Heart Dataset', fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "plt.figure(figsize=(12, 10))\n",
        "corr = heart_processed.corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\n",
        "plt.title('Correlation Matrix - Heart Dataset', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "for i, col in enumerate(categorical_cols[:4]):  # First 4 categorical columns\n",
        "    if col in heart_processed.columns:\n",
        "        sns.countplot(data=heart_processed, x=col, hue='target', ax=axes[i])\n",
        "        axes[i].set_title(f'Count of {col} by Target')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        # Rotate x labels if needed\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Categorical Features by Target - Heart Dataset (Part 1)', fontsize=16, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Second set of categorical features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(categorical_cols[4:]):  # Last 4 categorical columns\n",
        "    if col in heart_processed.columns:\n",
        "        sns.countplot(data=heart_processed, x=col, hue='target', ax=axes[i])\n",
        "        axes[i].set_title(f'Count of {col} by Target')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        # Rotate x labels if needed\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Categorical Features by Target - Heart Dataset (Part 2)', fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Feature Engineering and Data Preparation\n",
        "\n",
        "# Define features and target variables\n",
        "def prepare_data_for_modeling(df):\n",
        "    \"\"\"\n",
        "    Prepare dataset for modeling by separating features and target\n",
        "    and performing any necessary feature engineering\n",
        "    \"\"\"\n",
        "    # Identify target column (should be 'target')\n",
        "    target_col = 'target'\n",
        "    if target_col not in df.columns and 'condition' in df.columns:\n",
        "        target_col = 'condition'\n",
        "        \n",
        "    # Separate features and target\n",
        "    X = df.drop(target_col, axis=1)\n",
        "    y = df[target_col]\n",
        "    \n",
        "    # Drop any non-feature columns\n",
        "    columns_to_drop = []\n",
        "    for col in X.columns:\n",
        "        if col in ['id', 'dataset', 'patient_id']:\n",
        "            columns_to_drop.append(col)\n",
        "    \n",
        "    if columns_to_drop:\n",
        "        X = X.drop(columns_to_drop, axis=1)\n",
        "    \n",
        "    # Handle categorical features\n",
        "    # Get categorical columns\n",
        "    cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    # One-hot encode categorical features\n",
        "    if cat_cols:\n",
        "        X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Prepare data for each dataset\n",
        "X_heart, y_heart = prepare_data_for_modeling(heart_processed)\n",
        "X_uci, y_uci = prepare_data_for_modeling(uci_processed)\n",
        "X_cleveland, y_cleveland = prepare_data_for_modeling(cleveland_processed)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Heart dataset:\")\n",
        "print(f\"X shape: {X_heart.shape}, y shape: {y_heart.shape}\")\n",
        "print(\"\\nUCI dataset:\")\n",
        "print(f\"X shape: {X_uci.shape}, y shape: {y_uci.shape}\")\n",
        "print(\"\\nCleveland dataset:\")\n",
        "print(f\"X shape: {X_cleveland.shape}, y shape: {y_cleveland.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Split each dataset\n",
        "X_train_heart, X_test_heart, y_train_heart, y_test_heart = split_data(X_heart, y_heart)\n",
        "X_train_uci, X_test_uci, y_train_uci, y_test_uci = split_data(X_uci, y_uci)\n",
        "X_train_cleveland, X_test_cleveland, y_train_cleveland, y_test_cleveland = split_data(X_cleveland, y_cleveland)\n",
        "\n",
        "# Normalize/standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_heart_scaled = scaler.fit_transform(X_train_heart)\n",
        "X_test_heart_scaled = scaler.transform(X_test_heart)\n",
        "\n",
        "scaler_uci = StandardScaler()\n",
        "X_train_uci_scaled = scaler_uci.fit_transform(X_train_uci)\n",
        "X_test_uci_scaled = scaler_uci.transform(X_test_uci)\n",
        "\n",
        "scaler_cleveland = StandardScaler()\n",
        "X_train_cleveland_scaled = scaler_cleveland.fit_transform(X_train_cleveland)\n",
        "X_test_cleveland_scaled = scaler_cleveland.transform(X_test_cleveland)\n",
        "\n",
        "print(\"Data split and scaled successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "# Define a function to evaluate model performance\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, dataset_name):\n",
        "    \"\"\"\n",
        "    Train and evaluate a model on the given dataset\n",
        "    \"\"\"\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    \n",
        "    # Get probability predictions for ROC curve\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    else:\n",
        "        fpr, tpr, roc_auc = None, None, None\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"Performance on {dataset_name} dataset:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot ROC curve if available\n",
        "    if fpr is not None and tpr is not None:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Receiver Operating Characteristic - {dataset_name}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    \n",
        "    return model, accuracy, report, fpr, tpr, roc_auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate Random Forest on Heart dataset\n",
        "print(\"Training Random Forest on Heart dataset...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model, rf_accuracy, rf_report, rf_fpr, rf_tpr, rf_auc = evaluate_model(\n",
        "    rf_model, X_train_heart_scaled, X_test_heart_scaled, y_train_heart, y_test_heart, \"Heart\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate XGBoost on Heart dataset\n",
        "print(\"Training XGBoost on Heart dataset...\")\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model, xgb_accuracy, xgb_report, xgb_fpr, xgb_tpr, xgb_auc = evaluate_model(\n",
        "    xgb_model, X_train_heart_scaled, X_test_heart_scaled, y_train_heart, y_test_heart, \"Heart\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate LightGBM on Heart dataset\n",
        "print(\"Training LightGBM on Heart dataset...\")\n",
        "lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "lgb_model, lgb_accuracy, lgb_report, lgb_fpr, lgb_tpr, lgb_auc = evaluate_model(\n",
        "    lgb_model, X_train_heart_scaled, X_test_heart_scaled, y_train_heart, y_test_heart, \"Heart\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models performance\n",
        "models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
        "accuracies = [rf_accuracy, xgb_accuracy, lgb_accuracy]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=models, y=accuracies)\n",
        "plt.title('Model Comparison - Heart Dataset')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves for all models in one plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(rf_fpr, rf_tpr, label=f'Random Forest (AUC = {rf_auc:.3f})', color='blue')\n",
        "plt.plot(xgb_fpr, xgb_tpr, label=f'XGBoost (AUC = {xgb_auc:.3f})', color='red')\n",
        "plt.plot(lgb_fpr, lgb_tpr, label=f'LightGBM (AUC = {lgb_auc:.3f})', color='green')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Explanation with SHAP\n",
        "\n",
        "# Get the best model (assuming XGBoost performed the best)\n",
        "best_model = xgb_model  # Replace with the best model based on results\n",
        "\n",
        "# Calculate SHAP values\n",
        "explainer = shap.TreeExplainer(best_model)\n",
        "shap_values = explainer.shap_values(X_test_heart_scaled)\n",
        "\n",
        "# Convert test data back to DataFrame with feature names for better visualization\n",
        "X_test_heart_df = pd.DataFrame(X_test_heart_scaled, columns=X_heart.columns)\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_test_heart_df, plot_type=\"bar\", show=False)\n",
        "plt.title(\"Feature Importance Using SHAP Values\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed SHAP summary plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_values, X_test_heart_df, show=False)\n",
        "plt.title(\"SHAP Summary Plot\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Dependence Plots for top features\n",
        "# Get top 3 features based on SHAP values\n",
        "feature_importance = pd.DataFrame(\n",
        "    np.abs(shap_values).mean(0),\n",
        "    index=X_heart.columns,\n",
        "    columns=['importance']\n",
        ").sort_values('importance', ascending=False)\n",
        "\n",
        "top_features = feature_importance.head(3).index.tolist()\n",
        "\n",
        "# Create dependence plots for top 3 features\n",
        "for feature in top_features:\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    shap.dependence_plot(feature, shap_values, X_test_heart_df, show=False)\n",
        "    plt.title(f\"SHAP Dependence Plot for {feature}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Feature Importance from Random Forest\n",
        "\n",
        "# Get feature importances from Random Forest model\n",
        "rf_importances = pd.DataFrame({\n",
        "    'Feature': X_heart.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot top 15 features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=rf_importances.head(15))\n",
        "plt.title('Feature Importances from Random Forest')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "# Hyperparameter tuning for XGBoost model\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Use a subset of hyperparameters to save computation time\n",
        "param_grid_small = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "print(\"Performing hyperparameter tuning for XGBoost. This may take a while...\")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb.XGBClassifier(random_state=42),\n",
        "    param_grid=param_grid_small,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_heart_scaled, y_train_heart)\n",
        "\n",
        "# Print best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"\\nEvaluating the tuned XGBoost model...\")\n",
        "tuned_xgb_model, tuned_xgb_accuracy, tuned_xgb_report, tuned_xgb_fpr, tuned_xgb_tpr, tuned_xgb_auc = evaluate_model(\n",
        "    best_xgb_model, X_train_heart_scaled, X_test_heart_scaled, y_train_heart, y_test_heart, \"Heart (Tuned XGBoost)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Cross-Dataset Evaluation\n",
        "\n",
        "# Test the best model trained on heart dataset on the other datasets\n",
        "print(\"Testing the best model on Cleveland dataset...\")\n",
        "cleveland_pred = best_xgb_model.predict(X_test_cleveland_scaled)\n",
        "cleveland_accuracy = accuracy_score(y_test_cleveland, cleveland_pred)\n",
        "cleveland_report = classification_report(y_test_cleveland, cleveland_pred)\n",
        "\n",
        "print(f\"Accuracy on Cleveland dataset: {cleveland_accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(cleveland_report)\n",
        "\n",
        "# Create confusion matrix for Cleveland dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test_cleveland, cleveland_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Cleveland Dataset (with Heart-trained model)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Test on UCI dataset\n",
        "print(\"\\nTesting the best model on UCI dataset...\")\n",
        "uci_pred = best_xgb_model.predict(X_test_uci_scaled)\n",
        "uci_accuracy = accuracy_score(y_test_uci, uci_pred)\n",
        "uci_report = classification_report(y_test_uci, uci_pred)\n",
        "\n",
        "print(f\"Accuracy on UCI dataset: {uci_accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(uci_report)\n",
        "\n",
        "# Create confusion matrix for UCI dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test_uci, uci_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - UCI Dataset (with Heart-trained model)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save the Best Model for Future Use\n",
        "\n",
        "# Save the best model using pickle\n",
        "import pickle\n",
        "\n",
        "# Save the best model\n",
        "model_filename = 'best_heart_disease_model.pkl'\n",
        "pickle.dump(best_xgb_model, open(model_filename, 'wb'))\n",
        "\n",
        "# Save the scaler\n",
        "scaler_filename = 'heart_scaler.pkl'\n",
        "pickle.dump(scaler, open(scaler_filename, 'wb'))\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "print(f\"Scaler saved to {scaler_filename}\")\n",
        "\n",
        "# Example of how to load and use the saved model\n",
        "print(\"\\nExample of loading and using the saved model:\")\n",
        "print(\"```python\")\n",
        "print(\"import pickle\")\n",
        "print(\"# Load the model and scaler\")\n",
        "print(\"loaded_model = pickle.load(open('best_heart_disease_model.pkl', 'rb'))\")\n",
        "print(\"loaded_scaler = pickle.load(open('heart_scaler.pkl', 'rb'))\")\n",
        "print(\"# Prepare new data and make prediction\")\n",
        "print(\"new_data = pd.DataFrame(...) # Your new data\")\n",
        "print(\"new_data_scaled = loaded_scaler.transform(new_data)\")\n",
        "print(\"prediction = loaded_model.predict(new_data_scaled)\")\n",
        "print(\"probability = loaded_model.predict_proba(new_data_scaled)\")\n",
        "print(\"```\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Make Predictions on New Data\n",
        "\n",
        "# Create a function to make predictions on new data\n",
        "def predict_heart_disease(data, model, scaler):\n",
        "    \"\"\"\n",
        "    Make heart disease predictions on new data\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas DataFrame\n",
        "        New patient data with the same features as the training data\n",
        "    model : trained model\n",
        "        The trained model to use for predictions\n",
        "    scaler : fitted scaler\n",
        "        The scaler used to standardize the training data\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array\n",
        "        Binary predictions (0: No disease, 1: Disease)\n",
        "    probabilities : numpy array\n",
        "        Probability of heart disease\n",
        "    \"\"\"\n",
        "    # Scale the data\n",
        "    scaled_data = scaler.transform(data)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = model.predict(scaled_data)\n",
        "    probabilities = model.predict_proba(scaled_data)[:, 1]\n",
        "    \n",
        "    return predictions, probabilities\n",
        "\n",
        "# Example: Create a sample patient data\n",
        "print(\"Example: Prediction for a new patient\")\n",
        "sample_patient = pd.DataFrame({\n",
        "    'age': [65],\n",
        "    'sex': [1],  # Male\n",
        "    'cp': [2],   # Chest pain type\n",
        "    'trestbps': [145],  # Resting blood pressure\n",
        "    'chol': [240],  # Cholesterol\n",
        "    'fbs': [1],  # Fasting blood sugar > 120 mg/dl\n",
        "    'restecg': [0],  # Resting ECG\n",
        "    'thalach': [150],  # Max heart rate achieved\n",
        "    'exang': [0],  # Exercise induced angina\n",
        "    'oldpeak': [2.3],  # ST depression\n",
        "    'slope': [0],  # Slope of peak exercise ST segment\n",
        "    'ca': [1],  # Number of major vessels colored by fluoroscopy\n",
        "    'thal': [3]  # Thalassemia\n",
        "})\n",
        "\n",
        "# Make prediction\n",
        "# Check if the sample has the same columns as the training data\n",
        "if set(sample_patient.columns) != set(X_heart.columns):\n",
        "    print(\"Warning: Sample data columns don't match training data columns.\")\n",
        "    print(f\"Sample columns: {sample_patient.columns}\")\n",
        "    print(f\"Training columns: {X_heart.columns}\")\n",
        "    print(\"Adjust the sample data or model before making predictions.\")\n",
        "else:\n",
        "    prediction, probability = predict_heart_disease(sample_patient, best_xgb_model, scaler)\n",
        "    \n",
        "    print(f\"Prediction: {'Heart Disease' if prediction[0] == 1 else 'No Heart Disease'}\")\n",
        "    print(f\"Probability of Heart Disease: {probability[0]:.4f}\")\n",
        "    \n",
        "    # Create a visualization for the prediction probability\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    plt.barh(['Heart Disease Risk'], [probability[0]*100], color='coral' if probability[0] > 0.5 else 'skyblue')\n",
        "    plt.xlim(0, 100)\n",
        "    plt.xlabel('Probability (%)')\n",
        "    plt.title('Heart Disease Risk Assessment')\n",
        "    for i, v in enumerate([probability[0]*100]):\n",
        "        plt.text(v + 3, i, f\"{v:.1f}%\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Conclusion and Future Work\n",
        "\n",
        "# Summarize findings\n",
        "print(\"# Summary of Heart Disease Analysis Project\")\n",
        "print(\"\\n## Key Findings:\")\n",
        "print(\"1. Several machine learning models were evaluated, with XGBoost showing the best performance\")\n",
        "print(\"2. The tuned XGBoost model achieved an accuracy of approximately {:.1f}% on the test set\".format(tuned_xgb_accuracy*100))\n",
        "print(\"3. Most important features for heart disease prediction include:\")\n",
        "for i, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"   - {i}: {row['importance']:.4f}\")\n",
        "    \n",
        "print(\"\\n## Model Performance Across Datasets:\")\n",
        "print(f\"- Heart dataset: {tuned_xgb_accuracy:.4f}\")\n",
        "print(f\"- Cleveland dataset: {cleveland_accuracy:.4f}\")\n",
        "print(f\"- UCI dataset: {uci_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n## Future Work:\")\n",
        "print(\"1. Collect more data to improve model generalization\")\n",
        "print(\"2. Explore additional feature engineering techniques\")\n",
        "print(\"3. Test more advanced models like neural networks\")\n",
        "print(\"4. Create a web application for heart disease risk assessment\")\n",
        "print(\"5. Incorporate additional medical parameters for improved accuracy\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
